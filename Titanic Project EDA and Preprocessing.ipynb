{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files:\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the data:\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data analysis:\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type analysis:\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Ticket' Column:\n",
    "Since there are a lot of Ticket duplicates we can assume that passengers that share the same ticket are friends or family. Let's make a new category for passengers that share ticket.\n",
    "\n",
    "Because the data is separated to train and test, some passengers that appear in the trian data can be sharing a ticket with passengers in the test data. Therefore, only for this feature manipulation we will combine the 'Ticket' column from both train and test set in to one list, and then save only the ticket names that are duplicated. Than we will make a list of dup tickets for the train and test sats separately. It is maybe not the right way of data preprocessing, but we are not building the model based on the test predictions, and also the goal is to achieve the highest score.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extruct 'Ticket' column from train set \n",
    "Ticket_train = train.Ticket.tolist()\n",
    "# Extruct 'Ticket' column from test set \n",
    "Ticket_test = test.Ticket.tolist()\n",
    "# Combine both lists.\n",
    "Ticket_total = Ticket_train + Ticket_test\n",
    "# Make a list of shared tickets:\n",
    "Ticket_total_Dup = set([x for x in Ticket_total if Ticket_total.count(x) > 1])\n",
    "# Save to new list shared tickets that appear only in train set. \n",
    "Ticket_Dup_train = list(set(Ticket_train) & set(Ticket_total_Dup))\n",
    "# Save to new list shared tickets that appear only in test set, later on. \n",
    "Ticket_Dup_test = list(set(Ticket_test) & set(Ticket_total_Dup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column for Shared ticket - Yes(1) or No(0)\n",
    "train['Share_ticket'] = 0\n",
    "\n",
    "# Create a mask for rows with shared ticket\n",
    "mask2 = train['Ticket'].isin(Ticket_Dup_train)\n",
    "\n",
    "# Apply mask on new column and assign 1 for shared ticket\n",
    "train.loc[mask2,'Share_ticket'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Title'  Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the 'Name' column we can extruct the Title of the passenger: \n",
    "train['Title'] = [row.split()[1] for row in train.Name]\n",
    "\n",
    "# First we will change every Title that is not ['Mr.', 'Mrs.', 'Miss.', 'Master.'] in to 'M':\n",
    "Mtitle = train.Title.value_counts() < 10\n",
    "train.loc[train['Title'].map(Mtitle),'Title'] = 'M'\n",
    "\n",
    "# We will calculate the average 'Age' for Miss. and Mrs. respectively\n",
    "Miss_avg_age = train[train['Title'] == 'Miss.']['Age'].dropna().mean()\n",
    "Mrs_avg_age = train[train['Title'] == 'Mrs.']['Age'].dropna().mean()\n",
    "\n",
    "# Write a function that changes the value 'M' according to 'Sex':\n",
    "# If male - change to 'Mr.', and if female - change to 'Mrs.' or 'Miss.' depends on the age.\n",
    "def Change_title(df,row,i):\n",
    "    if row == 'M':\n",
    "        if df.iloc[i]['Sex'] == 'female':\n",
    "            age = df.iloc[i]['Age']\n",
    "            # If a woman age is closer to the average 'Miss.' age so assign 'Miss', else assign 'Mrs.':\n",
    "            if abs(Miss_avg_age-age) < abs(Mrs_avg_age-age): \n",
    "                row = 'Miss.'\n",
    "            else:\n",
    "                row = 'Mrs.'    \n",
    "            return(row)\n",
    "        else:\n",
    "            row = 'Mr.'\n",
    "            return(row)\n",
    "    else:\n",
    "        return(row)\n",
    "\n",
    "# Apply function on 'Title' column with list comprehension:\n",
    "train['Title'] = [Change_title(train,row,i) for i, row in enumerate(train['Title'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Cabin' Column:\n",
    "The 'Cabin' column was full of NaN's and was really messy, the values were displayed as \"Capital Letter\"+\"number\". \n",
    "Since the number is not important, but the Cabin Letter is, we will first extruct the letters to a different column \"Cabin_Class\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Class_L(row):\n",
    "    if type(row) == str:\n",
    "        row = row[0]\n",
    "        return(row)\n",
    "    else: \n",
    "        return(row) \n",
    "\n",
    "# Apply function on 'Cabin'\n",
    "train['Cabin_Class'] = [Class_L(row) for row in train['Cabin']]\n",
    "\n",
    "# A Class 'T' appeared only in one row, apparently becouse of typo. Let's change it to the most common Class within the \n",
    "# group of Pclass=1 - 'C':  \n",
    "train['Cabin_Class'] = train['Cabin_Class'].replace('T','C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns Plotting:\n",
    "Before dealing with the missing values problem in the data, les't first explore the columns in the data.\n",
    "\n",
    "We will begin with displaying counterplots of the categorical columns of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_Columns = ['Survived','Pclass','Title','SibSp','Parch','Embarked','Cabin_Class','Share_ticket']\n",
    "sns.set(style = \"darkgrid\")\n",
    "fig = plt.figure(figsize = (18, 6))\n",
    "fig.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "\n",
    "for i ,col in enumerate(Cat_Columns):\n",
    "    ax = fig.add_subplot(2, 4, i + 1)\n",
    "    sns.countplot(x = col,data = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Insights: \n",
    "- We can clearly see that more passengers died then survived, most of the passengers stayed in the lower class (3), and most of the passengers embarked at 'S'.\n",
    "- Since Mr. and Master. are related to males and Mrs. and Miss. are related to females, te is no need to plot the 'Sex' column.\n",
    "- Both 'SibSp' and 'Parch' have values that count really low, there for we will later fix it by summing all values from 2-8 in 'SibSp' and 2-6 in 'Parch'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the 'SibSp' and 'Parch' columns:\n",
    "for col in ['Parch','SibSp']:\n",
    "    train[col] = train[col].apply(lambda x: 2 if x > 2 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Analysis in 'Cabin' \n",
    "We want to check if there is a correlation between passengers that reserved a ticket together ['Share_Ticket'] == 1, and if those passengers are sharing the same cabin, so for example, if the Anderson family (5 people) have the same ticket, do they \n",
    "also have the same cabin...\n",
    "We will check it by looping over each group of passengers with the same ticket number, only on groups without missing values\n",
    "(nans), and we'll calculate the number of times a group with the same ticket also shared the same cabin, and the number of times a group with the same ticket were scattered over different cabins.\n",
    "\n",
    "This analysis will help us to understand how to impute the 'Cabin' missing values of passengers with the same ticket \n",
    "(['Share_Ticket'] == 1), So, therefore, if the above correlation exists, each group that shares the same ticket, and have\n",
    "missing values in the 'Cabin' column, will be imputed with the same 'Cabin' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of shared tickets: \n",
    "Tickets_Dup = train[train['Share_ticket'] == 1]['Ticket'].unique().tolist()\n",
    "\n",
    "def Check_unique_Cabins(df,Tickets_Dup):\n",
    "    \n",
    "    Count = {\n",
    "        'Unique Cabin' : 0,\n",
    "        'Different Cabin' : 0\n",
    "            }\n",
    "\n",
    "    for ticket in Tickets_Dup:\n",
    "        mask = df['Ticket'] == ticket\n",
    "        Ticket_Group = df.loc[mask,'Cabin_Class']\n",
    "        With_nan=Ticket_Group.isnull().values.sum()\n",
    "        if With_nan == 0:\n",
    "            a = len(Ticket_Group.unique())\n",
    "            if a == 1:\n",
    "                Count['Unique Cabin'] += 1\n",
    "            elif a > 1:\n",
    "                Count['Different Cabin'] += 1\n",
    "    return(Count)    \n",
    "print(Check_unique_Cabins(train,Tickets_Dup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, So we saw that there is a clear correlation between buying the same ticket and staying in the same cabin. \n",
    "Therefore, we are left with 3 different cases of missing data in the 'Cabin' column:\n",
    "\n",
    "Case 1: Group of passengers with the same ticket number where some have a cabin value and some doesn't (missing values).\n",
    "For those who doesn't have a cabin value we need to fill the missing values according to the cabin of the other passengers in the group.\n",
    "In the rare case were there is more then one unique cabin for a group, we'll fill with the most frequent value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Case_1(df,Tickets_Dup):\n",
    "    #Loop over the shared ticket numbers: \n",
    "    for ticket in Tickets_Dup:\n",
    "        mask = df['Ticket'] == ticket\n",
    "        Ticket_grp = df.loc[mask,'Cabin_Class']\n",
    "        # Count unique number of ticket in group:\n",
    "        tot = len(Ticket_grp.unique())\n",
    "        if tot > 1:\n",
    "            # Check if there is nan in the group (but also other value...)\n",
    "            tot_num = Ticket_grp.isnull().values.sum()\n",
    "            if tot_num > 0:\n",
    "                # Find the most frequent value that is not a nan:\n",
    "                freq_val = Ticket_grp.value_counts().index[0]\n",
    "                # replace nan in group with the most frequent value:\n",
    "                df.loc[mask ,'Cabin_Class'] = df[mask]['Cabin_Class'].replace(np.nan,freq_val)\n",
    "    return(df)\n",
    "\n",
    "train = Case_1(train,Tickets_Dup)\n",
    "print(Check_unique_Cabins(train,Tickets_Dup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see another 10 'Unique Cabin' where added as a result of filling the nan values in groups with other values also.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now left with two cases, the second one refers to the shared ticket groups that their cabin is completly unknown, and the third group is passengers that bought thier ticket not in a group, and thier cabin is unknown as well. \n",
    "To impute thier missing cabin values we need first to check some correlation between the 'Cabin' column to the other columns in the data. \n",
    "\n",
    "To compute the correlations between other columns with categorical data, we will use the Chi squared test of independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chi_sqrt(df,Columns,Col_def):   \n",
    "    \n",
    "    Chi_res = []\n",
    "    Cabins = df[df.Cabin_Class.notna()]\n",
    "    \n",
    "    for col in Columns:\n",
    "        crosstab = pd.crosstab(Cabins[Col_def], Cabins[col])\n",
    "        chi2 = stats.chi2_contingency(crosstab)[0]\n",
    "        Chi_res.append(chi2)\n",
    "        \n",
    "    zipped_lists = zip(Chi_res, Columns)\n",
    "    sorted_zipped_lists = sorted(zipped_lists,reverse=False)\n",
    "    Chi_res = [x for x,_  in sorted_zipped_lists]\n",
    "    Columns = [x for _,x  in sorted_zipped_lists]\n",
    "    \n",
    "    sns.set(style = \"darkgrid\")\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(Columns,Chi_res)\n",
    "    ax.set_yticklabels(Columns)\n",
    "    ax.set_xlabel('Chi Squared')\n",
    "    plt.title('Correlation with ' + Col_def)\n",
    "    plt.show()\n",
    "        \n",
    "    \n",
    "\n",
    "Columns = ['Pclass','SibSp','Parch','Embarked','Share_ticket']\n",
    "Col_def = 'Cabin_Class'\n",
    "Chi_sqrt(train,Columns,Col_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 'Pclass' column has the most correlation between all the other categorical columns. Let's explore it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass distribution over different Cabins\n",
    "train_Cabin_dropna = train.dropna(subset = ['Cabin_Class'])\n",
    "sns.set(style = \"darkgrid\")\n",
    "sns.catplot(x = \"Pclass\", col = \"Cabin_Class\",\n",
    "                data = train_Cabin_dropna, kind = \"count\",\n",
    "                height = 4, aspect = .7 ,col_order = ['A','B','C','D','E','F','G']);\n",
    "plt.show()\n",
    "\n",
    "# In the plot bellow we can see a counterplot that counts the number of Pclass apearance in each Cabin. \n",
    "# Remember, there is a lot of missing data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the Cabin missing data over each Pclass:\n",
    "train_Cabin_nan = train[train['Cabin_Class'].isnull()]\n",
    "sns.set(style = \"darkgrid\")\n",
    "sns.countplot(x = 'Pclass',data = train_Cabin_nan)\n",
    "plt.title('Cabin_Class Missing data distribution over different Pclass')\n",
    "plt.show()\n",
    "\n",
    "# As we can see, most of the Cabin missing data belongs to the lower Pclass, we need to find a relevant way to estimate the\n",
    "# missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Values Replacement: We decided To fill the missing values in the 'Cabin_Class' column in the following way:\n",
    "\n",
    "For each class of 'Pclass' (1/2/3) find the Cabin_Class values and thier apperence probabilities. Then, randomly assign \n",
    "classes to missing Cabin_Class values with the respective probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column that is to be imputed ['Cabin_Class'], and the column with the higer correlation that the probabiliteis\n",
    "# will be computed from ['Pclass']\n",
    "Col_with_nans = 'Cabin_Class'\n",
    "Col_prob_from = 'Pclass'\n",
    "\n",
    "# Create a Function that provides the lists of Cat values for each pclass and thier probabilities:\n",
    "def Create_prob(df):\n",
    "    # Column unique values (for Pclass is(1/2/3)):\n",
    "    col_un = np.sort(df[Col_prob_from].unique().tolist())\n",
    "    Cat_vals = {}\n",
    "    Cat_vals_proba = {}\n",
    "    for i in col_un:\n",
    "        mask = df[Col_prob_from] == i\n",
    "        # List of categorical values for each pclass group:\n",
    "        lst = df.loc[mask,Col_with_nans].value_counts().index.tolist()\n",
    "        # List of appearance for each value:\n",
    "        n = np.array(df.loc[mask,Col_with_nans].value_counts().tolist())\n",
    "        # Compute proba:\n",
    "        prob = n / (sum(n))\n",
    "        Cat_vals[i]=lst\n",
    "        Cat_vals_proba[i]=prob\n",
    "    return(Cat_vals,Cat_vals_proba)\n",
    "\n",
    "Cat_Pclass,Cat_Pclass_proba = Create_prob(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: we need to fill one unique value to the group of passengers who have bought the same ticket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that randomly assigns Cabin_Class values by grouping Pclass value and according to their probabilities\n",
    "# lists:   \n",
    "Tickets_Dup2 = train[train.Cabin_Class.isna() & train.Share_ticket == 1]['Ticket'].unique().tolist()\n",
    "\n",
    "def Case_2(df,Tickets_Dup2):\n",
    "    #Loop over the shared ticken numbers: \n",
    "    for ticket in Tickets_Dup2:\n",
    "        mask = df['Ticket'] == ticket\n",
    "        Ticket_grp = df.loc[mask]\n",
    "        # Count unique number of ticket in group:\n",
    "        tot = len(Ticket_grp)\n",
    "        # Obtaining the Pclass of the group:\n",
    "        col_val = int(Ticket_grp['Pclass'].unique()) \n",
    "        val = str(np.random.choice(Cat_Pclass[col_val],1,p = Cat_Pclass_proba[col_val])[0])\n",
    "        df.loc[mask,'Cabin_Class']=df[mask]['Cabin_Class'].replace(np.nan,val)   \n",
    "    return(df)\n",
    "\n",
    "train = Case_2(train,Tickets_Dup2)\n",
    "print(Check_unique_Cabins(train,Tickets_Dup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 3: Impute a random class to nan cabins for passengers that didn't buy the ticket as a group (['Share_ticket']==0). \n",
    "We will use the same permutation method used in case 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that randomly assigns Cabin_Class values by grouping Pclass value and according to their probabilities\n",
    "# lists:  \n",
    "def Assign_Cabin(row,i):\n",
    "    if type(row) == float:\n",
    "        pclass = train['Pclass'][i]\n",
    "        row = str(np.random.choice(Cat_Pclass[pclass],1,p=Cat_Pclass_proba[pclass])[0])\n",
    "    return(row)\n",
    "\n",
    "train['Cabin_Class']=[Assign_Cabin(row,i) for i, row in enumerate(train['Cabin_Class'])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass distribution over different Cabins after Estimating missing values\n",
    "sns.catplot(x=\"Pclass\", col=\"Cabin_Class\",\n",
    "                data=train, kind=\"count\",\n",
    "                height=4, aspect=.7,col_order=['A','B','C','D','E','F','G'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Fare' Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a swarmplot of the ticket price distribution ['Fare'] by each Cabin Class:\n",
    "sns.swarmplot(x='Cabin_Class', y='Fare', data=train, order=['A','B','C','D','E','F','G'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, there is an outlier in Cabin B. It will be fixed by assigning to it a new value that equals to the mean of\n",
    "# the other ticket prices in Cabin B, plus a 3 sigma. In this way it stays a high value but not an outlier. \n",
    "mask = train['Cabin_Class'] == 'B'\n",
    "Cabin_Class_B = train[mask]\n",
    "outlier = train[mask]['Fare'].max()\n",
    "Avg = Cabin_Class_B[Cabin_Class_B['Fare'] < outlier]['Fare'].mean()\n",
    "Std = Cabin_Class_B[Cabin_Class_B['Fare'] < outlier]['Fare'].std()\n",
    "train['Fare'] = train['Fare'].replace(outlier, Avg + 3 * Std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Embarked' Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked column - There are 2 missing values, we will fill them with the most frequent value 'S':\n",
    "train.Embarked.fillna('S',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Age_col' Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ages less than one year to be one\n",
    "babies = train['Age'] < 1\n",
    "train.loc[babies, \"Age\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'Age' column there are many missing values aswell. We want to check for correlations between the categorical columns in the data. To do that we need first to transform these column to categorical by assigning the ages to bin ranges. We will call the new column 'Age_col'. After that, we will use the Chi-squared function to check for correlations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ranges:\n",
    "ranges = [*range(0,90,10)]\n",
    "group_names = ['0-10','10-20', '20-30','30-40', '40-50', '50-60', '60-70','70-80']\n",
    "# Create categorical Age group column\n",
    "train['Age_col'] = pd.cut(train['Age'], bins = ranges, labels = group_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing categorical columns to comapre with:\n",
    "Columns = ['Pclass','SibSp','Parch','Embarked','Share_ticket','Title','Cabin_Class']\n",
    "Col_def = 'Age_col'\n",
    "# Applying Chi-squared:\n",
    "Chi_sqrt(train,Columns,Col_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 'Title' is highly correlated with 'Age_col', so we can impute the missing values with the probability imputer using the function 'Create_prob' to create the probabilities of the age according to the title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column that is to be imputed ['Age_col'], and the column with the higer correlation that the probabiliteis\n",
    "# will be computed from ['Title']\n",
    "Col_with_nans = 'Age_col'\n",
    "Col_prob_from = 'Title'\n",
    "\n",
    "# Convert 'Age_col' column from category to object - Create_prob function works better with object columns.\n",
    "train.Age_col=train.Age_col.astype('object')\n",
    "\n",
    "# Call the function and find probabilities: \n",
    "Cat_Title,Cat_Title_proba = Create_prob(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that randomly assigns 'Age_col' values by grouping 'Title' value and according to their probabilities\n",
    "# lists:  \n",
    "  \n",
    "def Assign_Age(df,row,i):\n",
    "    if type(row) == float:\n",
    "        title = df['Title'][i]\n",
    "        row = str(np.random.choice(Cat_Title[title],1,p=Cat_Title_proba[title])[0])\n",
    "    return(row)\n",
    "\n",
    "train['Age_col']=[Assign_Age(train,row,i) for i, row in enumerate(train['Age_col'])]\n",
    "\n",
    "\n",
    "# Let's plot a counterplot for the full imputer column - 'Age_col'\n",
    "sns.countplot(x='Age_col',data=train , order=group_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Age' column:\n",
    "Impute the missing values of age by the mean value of existing values for each age group in 'Age_col'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of 'Age_col' values by order:\n",
    "Age_ranges = list(np.sort(train.Age_col.unique()))\n",
    "\n",
    "# Define a function that computes the mean value for each age group in 'Age_col'. Age_means is a dictionary with columns of \n",
    "# 'Age_col' and thier coresponding mean value to be assigned in 'Age' missing values.\n",
    "def Age_mean(Age_ranges,df):\n",
    "    Age_means = {}\n",
    "    for age_bin in Age_ranges:\n",
    "        Age_means[age_bin] = round(df[df['Age_col'] == age_bin]['Age'].mean()) \n",
    "    return(Age_means)\n",
    "Age_means = Age_mean(Age_ranges,train)  \n",
    "\n",
    "# Impute missing values:\n",
    "train.loc[np.isnan(train['Age']),'Age'] =[Age_means[row['Age_col']] for i, row in train.iterrows() if np.isnan(row['Age'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous values with hist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_Columns = ['Age','Fare']\n",
    "sns.set(style = \"darkgrid\")\n",
    "fig = plt.figure(figsize = (12, 4))\n",
    "fig.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "\n",
    "for i ,col in enumerate(Cat_Columns):\n",
    "    ax = fig.add_subplot(1, 2, i + 1)\n",
    "    ax.set_xlabel(col)\n",
    "    plt.hist(x = col,data = train,bins = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Age' column is distributed close to normal - need scaling before modeling.\n",
    "- 'Fare' column is more exponential distributed, it will need some feature engineering before modeling - Log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation for Fare\n",
    "log = PowerTransformer()\n",
    "# Fit and Transform Log transformation:\n",
    "log.fit(train[['Fare']])\n",
    "train['Fare_log'] = log.transform(train[['Fare']])\n",
    "\n",
    "# Scaler for scaling the Age column:\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform Scaler:\n",
    "scaler.fit(train[['Age']])\n",
    "train['scaled_Age'] = scaler.transform(train[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_Columns = ['Age','Fare','scaled_Age','Fare_log']\n",
    "sns.set(style = \"darkgrid\")\n",
    "fig = plt.figure(figsize = (12, 8))\n",
    "fig.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "\n",
    "for i ,col in enumerate(Cat_Columns):\n",
    "    ax = fig.add_subplot(2, 2, i + 1)\n",
    "    ax.set_xlabel(col)\n",
    "    plt.hist(x = col,data = train,bins = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproccesing the Test set:\n",
    "Just before we start building a machine learning model to predict whether a passenger survived or not, we first need to apply the preproccesing transformations to the test set according what was done to the train set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data analysis:\n",
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Ticket' column:\n",
    "\n",
    "# Add a new column for Shared ticket - Yes(1) or No(0)\n",
    "test['Share_ticket'] = 0\n",
    "\n",
    "# Create a mask for rows with shared ticket\n",
    "mask = test['Ticket'].isin(Ticket_Dup_test)\n",
    "\n",
    "# Apply mask on new column and assign 1 for shared ticket\n",
    "test.loc[mask,'Share_ticket'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# 'Title' column:\n",
    "\n",
    "# From the 'Name' column we can extruct the Title of the passenger: \n",
    "test['Title'] = [row.split()[1] for row in test.Name]\n",
    "\n",
    "# First we will change every Title that is not ['Mr.', 'Mrs.', 'Miss.', 'Master.'] in to 'M':\n",
    "Mtitle = test.Title.value_counts() < 10\n",
    "test.loc[test['Title'].map(Mtitle),'Title'] = 'M'\n",
    "\n",
    "# We will use the average 'Age' of Miss. and Mrs. from what was calculated in the train set.\n",
    "# Apply function on 'Title' column with list comprehension:\n",
    "test['Title'] = [Change_title(test,row,i) for i, row in enumerate(test['Title'])]\n",
    "\n",
    "\n",
    "\n",
    "# 'SibSp' and 'Parch' columns\n",
    "\n",
    "# Fixing the 'SibSp' and 'Parch' columns:\n",
    "for col in ['Parch','SibSp']:\n",
    "    test[col] = test[col].apply(lambda x: 2 if x > 2 else x)\n",
    "\n",
    "    \n",
    "    \n",
    "# 'Cabin_Class' column:\n",
    "\n",
    "# Apply function on 'Cabin'\n",
    "test['Cabin_Class'] = [Class_L(row) for row in test['Cabin']]\n",
    "\n",
    "# Create a list of shared tickets: \n",
    "Tickets_Dup = test[test['Share_ticket'] == 1]['Ticket'].unique().tolist()\n",
    "\n",
    "# Case 1: Imputing mising values according to the same cabin for passengers in a group:\n",
    "test = Case_1(test,Tickets_Dup)\n",
    "\n",
    "# Case 2: Randomly assigns Cabin_Class values by Pclass values and according to their probabilities lists for groups with shared\n",
    "# ticket, and unknown cabin for all group:\n",
    "Tickets_Dup2 = test[test.Cabin_Class.isna() & test.Share_ticket == 1]['Ticket'].unique().tolist()\n",
    "test = Case_2(test,Tickets_Dup2)\n",
    "\n",
    "# Case 3: Randomly assigns Cabin_Class values by Pclass values and according to their probabilities lists for lone passengers:\n",
    "test['Cabin_Class'] = [Assign_Cabin(row,i) for i, row in enumerate(test['Cabin_Class'])] \n",
    "\n",
    "\n",
    "\n",
    "# 'Fare' column:\n",
    "\n",
    "# Fill missing value by mean value:\n",
    "test['Fare'] = test['Fare'].fillna(test['Fare'].mean())\n",
    "\n",
    "# Log transformation for for scalling to normal prob:\n",
    "test['Fare_log'] = log.transform(test[['Fare']])\n",
    "\n",
    "\n",
    "\n",
    "# 'Age' & 'Age_col' column:\n",
    "\n",
    "# Fix ages less than one year to be one\n",
    "test.loc[babies, \"Age\"] = 1\n",
    "# Create categorical Age group column\n",
    "test['Age_col'] = pd.cut(test['Age'], bins = ranges, labels = group_names)\n",
    "# Randomly assigns 'Age_col' values by grouping 'Title' value and according to their probabilities lists: \n",
    "test['Age_col'] = [Assign_Age(test,row,i) for i, row in enumerate(test['Age_col'])]\n",
    "# Impute mean values per Age_col group for Age column: \n",
    "test.loc[np.isnan(test['Age']),'Age'] = [Age_means[row['Age_col']] for i, row in test.iterrows() if np.isnan(row['Age'])]\n",
    "\n",
    "# Scaling transformation:\n",
    "test['scaled_Age'] = scaler.transform(test[['Age']])\n",
    "\n",
    "# Drop unused columns:\n",
    "test.drop(['Ticket', 'Cabin' , 'Name','Sex' , 'Age_col','Fare' , 'Age'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns:\n",
    "train.drop(['Ticket', 'Cabin' , 'Name','Sex' , 'Age_col','Fare' , 'Age'],inplace=True, axis=1)\n",
    "\n",
    "# View Final train set:\n",
    "train.to_csv('train_clean.csv',index = False)\n",
    "test.to_csv('test_clean.csv',index = False)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
